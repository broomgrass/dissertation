{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d98746b",
   "metadata": {},
   "source": [
    "# Remove Headers (for data capsule)\n",
    "\n",
    "This code is derived from my running headers experiments (4.RunningHeadersExperiments.ipynb), which is, in turn, derived from Ted Underwood's running headers code.\n",
    "\n",
    "Copied text from `4.RunningHeadersExperiments`:\n",
    "\n",
    "> Relies on using Ted Underwood's code for removing running headers\n",
    "\n",
    ">https://github.com/tedunderwood/DataMunging/blob/master/runningheaders/HeaderFinder.py\n",
    "\n",
    ">Note that the pagelist needs to be the strings on the pages in list format (for more information, look at how the pop function works).\n",
    "\n",
    ">Also, because the function uses .pop, it will modify the list of pages you give it, so if you run remove_headers a couple times to experiment and then can't figure out why you can get the modified text but not the list of headers, review your order of when you're looking at things.\n",
    "\n",
    ">Another aspect of pop to watch for is that if you have a page with only a few repeating elements, then the index may get out of range quickly.\n",
    "\n",
    ">For example, if your page is Table, Reis, 100, and the script considers that a running header, then if you .pop index 1, there will no longer be an index 2.\n",
    "\n",
    ">To get around this, I changed the sets in repeated into lists, maintaining the sublist order and the interior tuple order. However, within each sublist, I sorted the tuples so that the one with the highest index would be first in the sublist.\n",
    "\n",
    ">Example: [('TRADE of LISBON', 1), ('Year .', 2)] becomes [('Year .', 2), ('TRADE of LISBON', 1)], so that Year is removed before TRADE of LISBON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada21dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8731524",
   "metadata": {},
   "source": [
    "## Define HeaderFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HeaderFinder.py\n",
    "#\n",
    "# Scans a list of pages for running headers, which we understand as lines, near\n",
    "# the top of a page, that are repeated within the space of two pages,\n",
    "# in either direction. The two-page window is necessary because headers\n",
    "# are sometimes restricted to recto or verso. A very common pattern\n",
    "# involves different, alternating recto and verso headers. We also use\n",
    "# fuzzy matching to allow for OCR errors and other minor variation (e.g.\n",
    "# page numbers that may be roman numerals).\n",
    "#\n",
    "# Once headers are identified, they can be treated in a range of different\n",
    "# ways. The first of these functions is not concerned to *separate* the header\n",
    "# from the original text but only to identify it so that it can be given extra\n",
    "# weight in page classification. The second function actually removes them.\n",
    "\n",
    "# In principle, this could all be done for footers as well. I haven't cared, because\n",
    "# it wasn't a big problem in the 19c volumes I've worked with so far. That\n",
    "# could change!\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def find_headers(pagelist, romannumerals):\n",
    "\t'''Identifies repeated page headers and returns them as a list keyed to\n",
    "\toriginal page locations.'''\n",
    "\n",
    "\t# For very short documents, this is not a meaningful task.\n",
    "\n",
    "\tif len(pagelist) < 5:\n",
    "\t\treturn []\n",
    "\n",
    "\tfirsttwos = list()\n",
    "\t# We construct a list of the first two substantial lines on\n",
    "\t# each page. We ignore short lines and lines that are just numbers,\n",
    "\t# and don't go deeper than five lines in any event.\n",
    "\n",
    "\t# We transform lines in this process -- e.g, by removing digits.\n",
    "\t# If we were attempting to *remove* lines from the original text,\n",
    "\t# we would probably need to construct objects that package the transformed\n",
    "\t# line with information about its original location, so we could also\n",
    "\t# remove the original.\n",
    "\n",
    "\tfor page in pagelist:\n",
    "\t\tthesetwo = list()\n",
    "\t\tlinesaccepted = 0\n",
    "\n",
    "        # Cathy's note: at one point, I had .splitlines() after page\n",
    "        # however, that meant that texts with no headers weren't getting added\n",
    "        # instead, splitlines when you first import the text.\n",
    "\t\tfor idx, line in enumerate(page):\n",
    "\t\t\tif idx > 4:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif line.startswith('<') and line.endswith('>'):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tline = \"\".join([x for x in line if not x.isdigit()])\n",
    "\t\t\t# We strip all numeric chars before the length check.\n",
    "\n",
    "\t\t\tif line in romannumerals:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# That may not get all roman numerals, because of OCR junk, so let's\n",
    "\t\t\t# attempt to get them by shrinking them below the length limit. This\n",
    "\t\t\t# will also have the collateral benefit of reducing the edit distance\n",
    "\t\t\t# for headers that contain roman numerals.\n",
    "\t\t\tline = line.replace(\"iii\", \"\")\n",
    "\t\t\tline = line.replace(\"ii\", \"\")\n",
    "\t\t\tline = line.replace(\"xx\", \"\")\n",
    "\n",
    "\t\t\tif len(line) < 5:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlinesaccepted += 1\n",
    "\t\t\tthesetwo.append(line)\n",
    "\n",
    "\t\t\tif linesaccepted >= 2:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tfirsttwos.append(thesetwo)\n",
    "\n",
    "\t# Now our task is to iterate through the firsttwos, identifying lines that\n",
    "\t# repeat within a window, which we define as \"this page and the two previous\n",
    "\t# pages.\"\n",
    "\n",
    "\t# We're going to do this with a list of sets. That way we can add things\n",
    "\t# without risk of duplication. Otherwise, when we add headers to previous\n",
    "\t# pages, we're always going to be checking whether they were already added.\n",
    "\n",
    "\trepeated = list()\n",
    "\tfor i in range(len(firsttwos)):\n",
    "\t\tnewset = set()\n",
    "\t\trepeated.append(newset)\n",
    "\n",
    "\tfor index in range(2, len(firsttwos)):\n",
    "\t\t# We can be sure the 2 index is legal because we have previously filtered\n",
    "\t\t# short documents.\n",
    "\n",
    "\t\tindexedlines = firsttwos[index]\n",
    "\n",
    "\t\tfor j in range (index - 2, index):\n",
    "\n",
    "\t\t\tpreviouslines = firsttwos[j]\n",
    "\n",
    "\t\t\tfor lineA in indexedlines:\n",
    "\t\t\t\tfor lineB in previouslines:\n",
    "\t\t\t\t\ts = SequenceMatcher(None, lineA, lineB)\n",
    "\t\t\t\t\tsimilarity = s.ratio()\n",
    "\t\t\t\t\tif similarity > .8:\n",
    "\t\t\t\t\t\trepeated[index].add(lineA)\n",
    "\t\t\t\t\t\trepeated[j].add(lineB)\n",
    "\n",
    "\t# Now we have a list of sets that contain digit-stripped strings\n",
    "\t# representing headers, in original page order, with empty sets where no headers\n",
    "\t# were found. We want to convert this to a list of lists of individual tokens.\n",
    "\n",
    "\tlistoftokenstreams = list()\n",
    "\n",
    "\tfor thispageheaders in repeated:\n",
    "\t\tthisstream = []\n",
    "\t\tfor header in thispageheaders:\n",
    "\t\t\tthisstream.extend(header.split())\n",
    "\t\tlistoftokenstreams.append(thisstream)\n",
    "\n",
    "\treturn listoftokenstreams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a61dd4",
   "metadata": {},
   "source": [
    "## Define remove_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def remove_headers(pagelist, romannumerals):\n",
    "\t'''Identifies repeated page headers and removes them from\n",
    "\tthe pages; then returns the edited pagelist.'''\n",
    "\n",
    "\t# For very short documents, this is not a meaningful task.\n",
    "\n",
    "\tif len(pagelist) < 5:\n",
    "\t\treturn pagelist\n",
    "\n",
    "\tfirsttwos = list()\n",
    "\t# We construct a list of the first two substantial lines on\n",
    "\t# each page. We ignore short lines and lines that are just numbers,\n",
    "\t# and don't go deeper than five lines in any event.\n",
    "\n",
    "\t# We transform lines in this process -- e.g, by removing digits.\n",
    "\t# We also package them as tuples in order to preserve information\n",
    "\t# that will allow us to delete the lines identified as repeats.\n",
    "\n",
    "\tfor page in pagelist:\n",
    "\t\tthesetwo = list()\n",
    "\t\tlinesaccepted = 0\n",
    "\n",
    "\t\tfor idx, line in enumerate(page):\n",
    "\t\t\tif idx > 4:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif line.startswith('<') and line.endswith('>'):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tline = \"\".join([x for x in line if not x.isdigit()])\n",
    "\t\t\t# We strip all numeric chars before the length check.\n",
    "\n",
    "\t\t\tif line in romannumerals:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# That may not get all roman numerals, because of OCR junk, so let's\n",
    "\t\t\t# attempt to get them by shrinking them below the length limit. This\n",
    "\t\t\t# will also have the collateral benefit of reducing the edit distance\n",
    "\t\t\t# for headers that contain roman numerals.\n",
    "\t\t\tline = line.replace(\"iii\", \"\")\n",
    "\t\t\tline = line.replace(\"ii\", \"\")\n",
    "\t\t\tline = line.replace(\"xx\", \"\")\n",
    "\n",
    "\t\t\tif len(line) < 5:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlinesaccepted += 1\n",
    "\t\t\tthesetwo.append((line, idx))\n",
    "\n",
    "\t\t\tif linesaccepted >= 2:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tfirsttwos.append(thesetwo)\n",
    "\n",
    "\t# Now our task is to iterate through the firsttwos, identifying lines that\n",
    "\t# repeat within a window, which we define as \"this page and the two previous\n",
    "\t# pages.\"\n",
    "\n",
    "\t# We're going to do this with a list of sets. That way we can add things\n",
    "\t# without risk of duplication. Otherwise, when we add headers to previous\n",
    "\t# pages, we're always going to be checking whether they were already added.\n",
    "\n",
    "\trepeated = list()\n",
    "\tfor i in range(len(firsttwos)):\n",
    "\t\tnewset = set()\n",
    "\t\trepeated.append(newset)\n",
    "\n",
    "\tfor index in range(2, len(firsttwos)):\n",
    "\t\t# We can be sure the 2 index is legal because we have previously filtered\n",
    "\t\t# short documents.\n",
    "\n",
    "\t\tindexedlines = firsttwos[index]\n",
    "\n",
    "\t\tfor j in range (index - 2, index):\n",
    "\n",
    "\t\t\tpreviouslines = firsttwos[j]\n",
    "\n",
    "\t\t\tfor lineA in indexedlines:\n",
    "\t\t\t\tfor lineB in previouslines:\n",
    "\t\t\t\t\ts = SequenceMatcher(None, lineA[0], lineB[0])\n",
    "\t\t\t\t\t# The zero indexes above are just selecting the string part\n",
    "\t\t\t\t\t# of a string, index tuple.\n",
    "\n",
    "\t\t\t\t\tsimilarity = s.ratio()\n",
    "\t\t\t\t\tif similarity > .8:\n",
    "\t\t\t\t\t\trepeated[index].add(lineA)\n",
    "\t\t\t\t\t\trepeated[j].add(lineB)\n",
    "\n",
    "                        \n",
    "    # Now we have a list of sets that contain tuples\n",
    "    # representing headers, in original page order, with empty sets where no headers\n",
    "    # were found. We can now use the line indexes in the tuples to pop out the\n",
    "    # relevant lines.\n",
    "    \n",
    "    # I make into a list so I can sort and ensure larger index numbers go first\n",
    "    # otherwise, taking away an index 1 first will modify the list, and thus, any later removals\n",
    "    # this has seemed only relevant for texts with repeated material like tables or other content\n",
    "    \n",
    "\trepeatedList = [list(x) for x in repeated]\n",
    "\n",
    "\trepeatedSortedList = []\n",
    "\n",
    "\tfor sublist in repeatedList:\n",
    "\t\tsublist = sorted(sublist, key=lambda i:i[1],reverse=True)\n",
    "\t\trepeatedSortedList.append(sublist)\n",
    "\n",
    "    # and just to double check\n",
    "\tassert len(pagelist) == len(repeatedSortedList)\n",
    "\n",
    "    # make a list of what has been removed\n",
    "\tremoved = list()\n",
    "    \n",
    "    # note that page needs to be a list, not a string\n",
    "\n",
    "\tfor page, headerset in zip(pagelist, repeatedSortedList):\n",
    "\t#     print('text from first page' + page[0])\n",
    "\t#     print('text in list form' + str(page[0:10]))\n",
    "    \n",
    "    \n",
    "\t\tfor header in headerset:\n",
    "\t\t\tlineindex = header[1]\n",
    "\t\t\tremoved.append(page.pop(lineindex))\n",
    "\tfinalpages = [x for sublist in pagelist for x in sublist]\n",
    "\n",
    "\treturn finalpages, removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747c56c",
   "metadata": {},
   "source": [
    "## Define romannumeralsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "romannumeralsList = ['i', 'ii', 'iii', 'iviiii', 'v', 'vi', 'vii', 'viiiiix', 'ixviiii', 'x', \n",
    "                 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx', \n",
    "                 'xxi', 'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325a3e4",
   "metadata": {},
   "source": [
    "#  Execute the Code\n",
    "### reminder: change folder names, etc.\n",
    "also, unsure if doing it on whatever the os of the data capsule is will mess with things - keep an eye out.\n",
    "\n",
    "I have also modified this to work on a single folder instead of doing it over a series of folders, since hopefully the virtual machine will be able to handle it...surely it is better than my poor little laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af33f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get list of ht unzipped folders\n",
    "htFolderList = glob.glob('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_unzip\\\\*')\n",
    "\n",
    "# to track completion\n",
    "processedFolders = []\n",
    "removedHeaders = []\n",
    "\n",
    "# iterate through each folder\n",
    "\n",
    "# note, you may want to do this in chunks if you are worried about errors or an old laptop!\n",
    "\n",
    "for folder in htFolderList:\n",
    "    \n",
    "    folderID = os.path.basename(folder)\n",
    "    \n",
    "    processedFolders.append(folderID)\n",
    "\n",
    "    # in each zip file, create a list of the pages\n",
    "    pageFolderList = glob.glob(folder+'\\\\*\\\\*')\n",
    "\n",
    "    # pageList will hold the strings of each page  \n",
    "    pageList = []\n",
    "        \n",
    "# note: if you don't need to modify files (ie, remove pages with extra technical info),\n",
    "# then move down to main iteration.\n",
    "\n",
    "##########################################\n",
    "\n",
    "    # remove first four pages technical microfilm info from aeu.ark pages\n",
    "    if 'aeu.ark' in folderID:\n",
    "\n",
    "        for page in pageFolderList[4:]:\n",
    "            with open(page, 'r', encoding='utf-8') as f:\n",
    "                pageList.append(f.read().splitlines())\n",
    "\n",
    "        # print('opened pages, added to page list!')\n",
    "\n",
    "        # run remove_headers on the series of pages, then add the returned pages to a single string\n",
    "        processedText = remove_headers(pageList, romannumeralsList)\n",
    "        textWithNoHeaders = '\\n'.join(processedText[0])\n",
    "        # removedHeaders.append(processedText[1])\n",
    "\n",
    "        # print('remove headers complete!')\n",
    "\n",
    "        # write the text, with no headers, to a .txt file\n",
    "        # remember to modify the filepath with the text identifier\n",
    "\n",
    "        tempFile = open('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_runningheadersremoved\\\\'+folderID+'.txt','w', encoding='utf-8')\n",
    "        tempFile.write(textWithNoHeaders)\n",
    "        tempFile.close()          \n",
    "            \n",
    "            \n",
    "##########################################\n",
    "            \n",
    "    else:   \n",
    "        # main iteration (you can skip right to here if you don't need to remove aeu.ark tech data)     \n",
    "        # print('main iteration')   \n",
    "        \n",
    "        for page in pageFolderList:\n",
    "            with open(page, 'r', encoding='utf-8') as f:\n",
    "                pageList.append(f.read().splitlines())\n",
    "\n",
    "        # run remove_headers on the series of pages, then add the returned pages to a single string\n",
    "        processedText = remove_headers(pageList, romannumeralsList)\n",
    "        textWithNoHeaders = '\\n'.join(processedText[0])\n",
    "        # removedHeaders.append(processedText[1])\n",
    "\n",
    "        # print('remove headers complete!')\n",
    "\n",
    "        # write the text, with no headers, to a .txt file\n",
    "        # remember to modify the filepath with the text identifier\n",
    "\n",
    "        tempFile = open('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_runningheadersremoved\\\\'+folderID+'.txt','w', encoding='utf-8')\n",
    "        tempFile.write(textWithNoHeaders)\n",
    "        tempFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e6c67",
   "metadata": {},
   "source": [
    "## identify and combine the LongSFiles metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2243b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_LongSGlobbed = glob.glob('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_zip_ocrNorm\\\\**\\\\longSfiles.txt')\n",
    "# ht_LongSGlobbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "longSfiles = []\n",
    "\n",
    "for file in ht_LongSGlobbed:\n",
    "    with open(file, 'r') as f:\n",
    "        longSfiles.append(f.read())\n",
    "\n",
    "longSfilesText = ''.join(longSfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempFile = open('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\plaintext\\\\txt_ht\\\\longSfiles_ht.txt', \"w\")\n",
    "# tempFile.write(longSfilesText)\n",
    "# tempFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa6917",
   "metadata": {},
   "source": [
    "## Separate the vol metadata and plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16444f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_volGlobbed = glob.glob('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_zip_ocrNorm\\\\**\\\\*vol.tsv')\n",
    "len(ht_volGlobbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d34ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ht_volGlobbed:\n",
    "    shutil.copy(file, 'C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\plaintext\\\\txt_ht\\\\vol_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf853995",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_cleanGlobbed = glob.glob('C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\origFile\\\\ht_zip_ocrNorm\\\\**\\\\*clean.txt')\n",
    "len(ht_cleanGlobbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ht_cleanGlobbed:\n",
    "    shutil.copy(file, 'C:\\\\Users\\\\cathy\\\\Documents\\\\twdb_files\\\\plaintext\\\\txt_ht\\\\text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
